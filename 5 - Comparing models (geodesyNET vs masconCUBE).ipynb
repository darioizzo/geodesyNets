{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geodesyNET vs masconCUBE\n",
    "A geodesyNET is but a new representation of the mass density of an irregular body.\n",
    "\n",
    "One can see it as a parametrization of a continuous function $\\rho(x,y,z)$ whose parameters $\\eta_i$ (network weights and biases) are learned efficiently thanks to the SGD approach.\n",
    "\n",
    "The question arises on how it compares with respect to the state-of-the-art in geodesy techniques. While any comparison to existing methods is bound to be unfair as geodesyNETs are the only representation that can learn the body shape and its interior structure simultaneously, we can introduce an alternative model (we call masconCUBE) that can be compared in fairness to GeodesyNET.\n",
    "\n",
    "A masconCUBE is a cube full of mascons so that $N\\times N\\times N$ mascons $m_j$ are placed withing the unit volume $V$ in a regular grid. We may then consider the various masses $m_j$ as the parameters of a model to be learned from observations.\n",
    "\n",
    "Differently from a geodesyNET, a masconCUBE does not represent the body density continuously, but other than this important detail the two representations have similar properties and hence can be compared once the various parameters $\\eta_i$ for the network and $m_i$ for the masconCUBE are learned.\n",
    "\n",
    "![alt text](figures/masconCUBE.png)\n",
    "\n",
    "In this notebook we setup the learning procedure for a masconCUBE. Note that the value of the gravitational acceleration created by a masconCUBE at $\\mathbf r_i$ is:\n",
    "$$\n",
    "\\mathbf a_i = \\sum_{j=1}^{N^3} \\frac {m_j}{r_{ij}^3}{\\mathbf r_{ij}}\n",
    "$$\n",
    "which means that each measurment of a gravitational acceleration results in linear relation so that at the end putting all measurements together one may write:\n",
    "$$\n",
    "\\mathbf A \\mathbf m = \\mathbf b\n",
    "$$\n",
    "In most cases, when $N>20$ this system becomes to large to be solved and one must revert to alternative approaches, in particular gradient descent based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import our module containing helper functions\n",
    "import gravann\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pickle as pk\n",
    "import os\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "# pytorch\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib notebook\n",
    "\n",
    "# Ensure that changes in imported module (gravann most importantly) are autoreloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# If possible enable CUDA\n",
    "gravann.enableCUDA()\n",
    "gravann.fixRandomSeeds()\n",
    "device = os.environ[\"TORCH_DEVICE\"]\n",
    "print(\"Will use device \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and visualizing a mascon model\n",
    "For the purpose of this notebook we will be using, as ground truth, also a mascon model: the one of Eros.\n",
    "Note though that the procedure in generic and one could also use a polyhedral gravity gound truth or other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"models/Churyumov-Gerasimenko.mdl\"\n",
    "target = target.split(\".\")[0].split(\"/\")[1]\n",
    "\n",
    "# We load the ground truth (a mascon model of some body)\n",
    "with open(\"mascons/\"+target+\".pk\", \"rb\") as file:\n",
    "    mascon_points, mascon_masses, mascon_name = pk.load(file)\n",
    "    \n",
    "mascon_points = torch.tensor(mascon_points)\n",
    "mascon_masses = torch.tensor(mascon_masses)\n",
    "\n",
    "# Print some information on the loaded ground truth \n",
    "# (non-dimensional units assumed. All mascon coordinates are thus in -1,1 and the mass is 1)\n",
    "print(\"Name: \", mascon_name)\n",
    "print(\"Number of mascons: \", len(mascon_points))\n",
    "print(\"Total mass: \", sum(mascon_masses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we visualize the loaded ground truth\n",
    "gravann.plot_mascon(mascon_points, mascon_masses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the masconCUBE model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the number of mascons per side of our masconCUBE\n",
    "N=23\n",
    "print(\"MasconCUBE number of parameters is: \", N*N*N)\n",
    "# Here we define the sqrt(m_j) or model parameters\n",
    "mascon_masses_model = torch.ones((N*N*N,1), requires_grad=True)\n",
    "#mascon_masses_model = mascon_masses_model.clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the points of the masconCUBE\n",
    "X,Y,Z = torch.meshgrid(torch.linspace(-1,1, N), torch.linspace(-1,1, N), torch.linspace(-1,1, N), indexing='ij')\n",
    "X = X.reshape(-1,1)\n",
    "Y = Y.reshape(-1,1)\n",
    "Z = Z.reshape(-1,1)\n",
    "mascon_points_model = torch.concat((X,Y,Z), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of the batch size, i.e. number of points\n",
    "# where the ground truth is compared to the predicted acceleration\n",
    "# at each training epoch.\n",
    "batch_size = 1000\n",
    "\n",
    "# Loss. The normalized L1 loss. \n",
    "loss_fn = gravann.normalized_L1_loss\n",
    "\n",
    "# The sampling method to decide what points to consider in each batch.\n",
    "# In this case we sample points unifromly in a sphere and reject those that are inside the asteroid\n",
    "targets_point_sampler = gravann.get_target_point_sampler(batch_size, \n",
    "                                                         limit_shape_to_asteroid=\"3dmeshes/\"+target+\"_lp.pk\", \n",
    "                                                         method=\"spherical\", \n",
    "                                                         bounds=[0,1])\n",
    "\n",
    "# Here we set the optimizer\n",
    "learning_rate = 1e-1\n",
    "optimizer = torch.optim.Adam(params = [mascon_masses_model], lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.8, patience = 200, min_lr = 1e-8,verbose=True)\n",
    "\n",
    "# And init the best results\n",
    "best_loss = np.inf\n",
    "\n",
    "# When a new network is created we init empty training logs\n",
    "loss_log = []\n",
    "weighted_average_log = []\n",
    "running_loss_log = []\n",
    "# .. and we init a loss trend indicators\n",
    "weighted_average = deque([], maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRAINING LOOP (normal training, no use of any prior shape information)------------------------\n",
    "# This cell can be stopped and started again without loosing memory of the training nor its logs\n",
    "torch.cuda.empty_cache()\n",
    "# The main training loop\n",
    "for i in range(3000):\n",
    "    # Each ten epochs we resample the target points\n",
    "    if (i % 10 == 0):\n",
    "        target_points = targets_point_sampler()\n",
    "        # We compute the labels whenever the target points are changed\n",
    "        labels = gravann.ACC_L(target_points, mascon_points, mascon_masses)\n",
    "    \n",
    "    # We compute the values predicted by the neural density field\n",
    "    predicted = gravann.ACC_L(target_points, mascon_points_model, mascon_masses_model*mascon_masses_model)\n",
    "    \n",
    "    # We compute the scaling constant (k in the paper) used in the loss\n",
    "    c = torch.sum(predicted*labels)/torch.sum(predicted*predicted)\n",
    "    \n",
    "    # We compute the loss\n",
    "    loss = loss_fn(predicted.view(-1), labels.view(-1))\n",
    "    \n",
    "    # We store the model if it has the lowest fitness \n",
    "    # (this is to avoid losing good results during a run that goes wild)\n",
    "    if loss < best_loss:\n",
    "        best_model = deepcopy(mascon_masses_model)\n",
    "        best_loss = loss\n",
    "        print('New Best: ', loss.item())\n",
    "    \n",
    "    # Update the loss trend indicators\n",
    "    weighted_average.append(loss.item())\n",
    "    \n",
    "    # Update the logs\n",
    "    weighted_average_log.append(np.mean(weighted_average))\n",
    "    loss_log.append(loss.item())\n",
    "    \n",
    "    # Print every i iterations\n",
    "    if i % 25 == 0:\n",
    "        wa_out = np.mean(weighted_average)\n",
    "        print(f\"It={i}\\t loss={loss.item():.3e}\\t  weighted_average={wa_out:.3e}\\t  c={c:.3e}\")\n",
    "        \n",
    "    # Zeroes the gradient (necessary because of things)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Perform a step in LR scheduler to update LR\n",
    "    scheduler.step(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual value of the mascon masses is here obtained by squaring and normalizing\n",
    "final_mascon_masses = best_model*best_model/torch.sum(best_model*best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_mascon_masses,\"cube_masses_\"+target+\".tensor\")\n",
    "torch.save(mascon_points_model,\"cube_points_\"+target+\".tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mascon_masses = torch.load(\"cube_masses_\"+target+\".tensor\").detach()\n",
    "mascon_points_model = torch.load(\"cube_points_\"+target+\".tensor\").detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Units of Length (this is Eros) (depend on how the the mascon model was created .... )\n",
    "# These numbers matter for the final values quantitatively (but not qualitatively)\n",
    "L = {\n",
    "    \"Bennu.pk\": 352.1486930549145,\n",
    "    \"Bennu_nu.pk\": 352.1486930549145,\n",
    "    \"Churyumov-Gerasimenko.pk\": 3126.6064453124995, \n",
    "    \"Eros.pk\" : 20413.864850997925,\n",
    "    \"Itokawa.pk\": 350.438691675663,\n",
    "    \"Itokawa_nu.pk\": 350.438691675663,\n",
    "    \"Torus.pk\": 3126.6064453124995,\n",
    "    \"Hollow.pk\": 3126.6064453124995,\n",
    "    \"Hollow_nu.pk\": 3126.6064453124995,\n",
    "    \"Hollow2.pk\": 3126.6064453124995,\n",
    "    \"Hollow2_nu.pk\": 3126.6064453124995\n",
    "}\n",
    "\n",
    "R0 = {\n",
    "    \"Eros.pk\" : 16000,\n",
    "    \"Itokawa.pk\": 300,\n",
    "    \"Churyumov-Gerasimenko.pk\": 4300,\n",
    "    \"Bennu.pk\": 565\n",
    "}\n",
    "L = L[target + \".pk\"]\n",
    "R0 = R0[target + \".pk\"] / L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us generate all Stokes coefficients up to order 7 and degree 7 (a square model)\n",
    "l=7\n",
    "m=7\n",
    "stokesC_mascon, stokesS_mascon = gravann.mascon2stokes(mascon_points_model.cpu().numpy(), final_mascon_masses.view(-1).detach().cpu().numpy(), R0, l, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we first trim the small values (for visualization purposes)\n",
    "final_mascon_masses[final_mascon_masses<1e-5] = 0\n",
    "# Then we plot\n",
    "gravann.plot_mascon(mascon_points_model, final_mascon_masses.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative comparison with ground truth and geodestNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stokes/stokes_77_\"+target+\"_gt.pk\", \"rb\") as file:\n",
    "    stokesC_gt, stokesS_gt = pk.load(file)\n",
    "with open(\"stokes/stokes_77_\"+target+\"_gann.pk\", \"rb\") as file:\n",
    "    stokesC_gann, stokesS_gann = pk.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we compute the sorted indexes corresponding to the largest values\n",
    "idxs = np.dstack(np.unravel_index(np.argsort(-np.abs(stokesC_gt.ravel())), (8, 8)))[0]\n",
    "# We print to screen the three models\n",
    "print(f\"Harmonics \\tGround Truth \\tGeodesyNET \\tMasconCUBE\")\n",
    "abs_errs_gann = []\n",
    "abs_errs_mascon = []\n",
    "rel_errs_gann = []\n",
    "rel_errs_mascon = []\n",
    "\n",
    "N_largest = 30\n",
    "\n",
    "for n,pos in enumerate(idxs):\n",
    "    if n == 0:\n",
    "        continue\n",
    "    if n == N_largest+1:\n",
    "        break\n",
    "    abs_errs_gann.append(abs(stokesC_gt[l][m] - stokesC_gann[l][m]))\n",
    "    abs_errs_mascon.append(abs(stokesC_gt[l][m] - stokesC_mascon[l][m]))\n",
    "    if(stokesC_gt[l][m] > 0):\n",
    "        rel_errs_gann.append(abs(stokesC_gt[l][m] - stokesC_gann[l][m]) / stokesC_gt[l][m])\n",
    "        rel_errs_mascon.append(abs(stokesC_gt[l][m] - stokesC_mascon[l][m]) / stokesC_gt[l][m])\n",
    "    l,m = pos\n",
    "    print(f\"C_{l}{m}, \\t\\t{stokesC_gt[l][m]:2.3e}, \\t{stokesC_gann[l][m]:2.3e}, \\t{stokesC_mascon[l][m]:2.3e}\")\n",
    "\n",
    "print(\"geodesyNet Mean Abs. Err. =\",np.mean(abs_errs_gann))\n",
    "print(\"Mascon Mean Abs. Err. =\",np.mean(abs_errs_mascon))\n",
    "print(\"geodesyNet Mean Rel. Err. =\",np.mean(rel_errs_gann))\n",
    "print(\"Mascon Mean Rel. Err. =\",np.mean(rel_errs_mascon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gravann.validation_mascon(mascon_points_model,final_mascon_masses,\n",
    "                          mascon_points,mascon_masses, \n",
    "                          N=10000,asteroid_pk_path=\"3dmeshes/\"+target+\".pk\",\n",
    "#                           sampling_altitudes=[0.005,0.01,0.025],\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding: direct encoding (i.e. feeding the network directly with the Cartesian coordinates in the unit hypercube)\n",
    "# was found to work well in most cases. But more options are implemented in the module.\n",
    "encoding = gravann.direct_encoding()\n",
    "\n",
    "# The model is here a SIREN network (FFNN with sin non linearities and a final absolute value to predict the density)\n",
    "model = gravann.init_network(encoding, hidden_layers=3,n_neurons=100, model_type=\"siren\", activation = gravann.AbsLayer())\n",
    "# Uncomment to run on CPU\n",
    "#model.load_state_dict(torch.load(\"models/eros.mdl\", map_location=torch.device('cpu')))\n",
    "model.load_state_dict(torch.load(\"models/\"+target+\".mdl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the Validation table with rel and abs errors on the predicted acceleration (w.r.t. ground truth) \n",
    "# at low, med, high altitudes (see paper). is requires sampling quite a lot, so it takes time \n",
    "gravann.validation(model, encoding, mascon_points, mascon_masses, use_acc=True, \n",
    "                   asteroid_pk_path=\"3dmeshes/\"+target+\".pk\", N_integration=500000, N=10000, progressbar=True,\n",
    "                   #sampling_altitudes=[0.005,0.01,0.025]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
