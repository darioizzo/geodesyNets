{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an ANN to represent the gravity produced by a spherical harmonics model\n",
    "In this notebook we explore the possibility to use ANNs to represent the generic shape and density of an irregular body represented by a spherical harmonics model. The error is defined on the gravitational acceleration.\n",
    "\n",
    "To run this notebook (and the others) create a conda environment using the following commands:\n",
    "```\n",
    " conda create -n geodesyann python=3.8 ipython scikit-learn numpy h5py matplotlib jupyter\n",
    " conda activate geodesyann\n",
    " conda install -c pytorch pytorch\n",
    " conda install -c open3d-admin open3d\n",
    " pip install sobol_seq\n",
    " pip install tetgen\n",
    " conda install pyvista pyvistaqt\n",
    "```\n",
    "\n",
    "To use CUDA, you will need to run\n",
    "```\n",
    " conda install pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# core stuff\n",
    "import gravann\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os\n",
    "\n",
    "# pytorch\n",
    "from torch import nn\n",
    "import torch\n",
    "# For debugging and development purposes this is now set to float64 ... change for speed on GPUs\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "# plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib notebook\n",
    "\n",
    "# Ensure that changes in imported module (gravann most importantly) are autoreloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# If possible enable CUDA\n",
    "gravann.enableCUDA()\n",
    "device = os.environ[\"TORCH_DEVICE\"]\n",
    "print(\"Will use device \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Instantiating the network\n",
    "The networks inputs are the cartesian coordinates of a point in the unit cube, encoded via some transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding choosen\n",
    "encoding = gravann.directional_encoding()\n",
    "\n",
    "# Network initialization scheme (note that if xavier uniform is used all outputs will start at, roughly 0.5)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.uniform_(m.bias.data, -0.0, 0.0)\n",
    "\n",
    "# Network architecture. Note that the dimensionality of the first linear layer must match the output\n",
    "# of the encoding chosen\n",
    "n_neurons = 100\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(encoding.dim,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,1),\n",
    "          nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "\n",
    "# Applying our weight initialization\n",
    "_  = model.apply(weights_init)\n",
    "\n",
    "# When a new network is created we init empty training logs\n",
    "loss_log = []\n",
    "weighted_average_log = []\n",
    "running_loss_log = []\n",
    "n_inferences = []\n",
    "\n",
    "# .. and we init some loss trend indicators\n",
    "from collections import deque\n",
    "weighted_average = deque([], maxlen=20)\n",
    "running_loss = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU NOW WANT TO LOAD THE ALREADY TRAINED NETWORK UNCOMMENT HERE.\n",
    "## It is important that the network architecture is compatible, otherwise this will fail\n",
    "#model.load_state_dict(torch.load(\"models/\" + name + \"_\" + encoding.name + \"_ACC.mdl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing an asteroid represented by the network\n",
    "The network output is the density in the unit cube. It is, essentially, a three dimensional function and as such it is difficult to plot. \n",
    "\n",
    "### Approach 1: plotting a grid of points colored with the rho value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravann.plot_model_grid(model, encoding, views_2d=False)\n",
    "plt.title(\"Believe it or not I am an asteroid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The ANN to match the ground truth potential\n",
    "\n",
    "Let it run up to when its < 1e-3 to actually see something that resembles the original asteroid. When stuck increase the number of monte carlo samples or play around the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTAL SETUP ------------------------------------------------------------------------------------\n",
    "# Here we set some hyperparameters\n",
    "N_mc_points = 200000\n",
    "batch_size = 1000\n",
    "\n",
    "# Here we set the loss\n",
    "loss_fn = gravann.mse_loss\n",
    "\n",
    "# Here we set the choosen Integration method\n",
    "mc_method = gravann.ACC_ld\n",
    "#mc_method = gravann.U_trap_opt\n",
    "\n",
    "# Here we set the method to sample the target points\n",
    "targets_point_sampler = gravann.get_target_point_sampler(batch_size, method=\"cubical\", scale_bounds = [1.0, 1.1])\n",
    "\n",
    "# Here we set the optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.5, patience = 5000, min_lr = 5e-6,verbose=True)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9,nesterov=True)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0005, max_lr=0.5, step_size_up=250, mode=\"exp_range\", gamma = .997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell can be stopped and started again without loosing memory of the training nor its indicators\n",
    "import pykep\n",
    "r, mu, c, s, n, m = pykep.util.load_gravity_model('spherical_harmonics/eros16.txt')\n",
    "\n",
    "# The main training loop\n",
    "for i in range(10000):\n",
    "    targets = targets_point_sampler()\n",
    "    # Labels are created from a spherical harmonics model\n",
    "    acc = pykep.util.gravity_spherical_harmonic(np.array(targets.cpu()*r*1.5), r, 1000000000, c, s, 16, 16)\n",
    "    labels = torch.tensor(acc, dtype=torch.float32)\n",
    "    # Compute the loss (use N=3000 to start with, then, eventually, beef it up to 200000)\n",
    "    predicted = mc_method(targets, model, encoding, N=N_mc_points)\n",
    "    loss = loss_fn(predicted.view(-1), labels.view(-1))\n",
    "    # Update the loss trend indicators\n",
    "    running_loss = 0.9 * running_loss + 0.1 * loss.item()\n",
    "    weighted_average.append(loss.item())\n",
    "    # Update the logs\n",
    "    running_loss_log.append(running_loss)\n",
    "    weighted_average_log.append(np.mean(weighted_average))\n",
    "    loss_log.append(loss.item())\n",
    "    n_inferences.append((N_mc_points*batch_size) // 1000)\n",
    "    # Print every i iterations\n",
    "    if i % 25 == 0:\n",
    "        wa_out = np.mean(weighted_average)\n",
    "        print(f\"It={i}\\t loss={loss.item():.8f}\\t  weighted_average={wa_out:.8f}\\t  running_loss={running_loss:.8f}\")\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Perform a step in LR scheduler to update LR\n",
    "    scheduler.step(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gravann.plot_model_rejection(model, encoding, views_2d=True, bw=True, N=50000, alpha=0.1, s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the loss history\n",
    "plt.figure()\n",
    "abscissa = np.cumsum(n_inferences)\n",
    "plt.semilogy(abscissa, loss_log)\n",
    "plt.semilogy(abscissa, running_loss_log)\n",
    "plt.semilogy(abscissa, weighted_average_log)\n",
    "plt.xlabel(\"Thousands of model evaluations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\",\"Running Loss\",\"Weighted Average Loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save to models/cluster_xxxx\n",
    "#torch.save(model.state_dict(), \"models/\" + name + \"_\" + encoding.name + \"_ACC.mdl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
