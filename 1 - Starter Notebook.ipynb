{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Notebook\n",
    "This notebook contains the main code to train a GeodesyNet on accelerations synthetically generated from a mascon model.\n",
    "At the end of the training a quick look at the resulting neural density field is also given.\n",
    "\n",
    "To run this notebook we recommend to create a conda environment using the following commands:\n",
    "```\n",
    " conda create -n geodesynets python=3.8 ipython numpy scipy scikit-learn  h5py matplotlib jupyter tqdm pandas\n",
    " conda activate geodesynets\n",
    " conda install -c pytorch pytorch\n",
    " conda install -c open3d-admin open3d\n",
    " pip install sobol_seq\n",
    " pip install tetgen\n",
    " conda install pyvista pyvistaqt\n",
    "```\n",
    "\n",
    "To use CUDA, you will need to run\n",
    "```\n",
    " conda install pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
    "```\n",
    "\n",
    "On CPU the training will still work but be slow, so use less points for the numercial quadrature and smaller batches. If you have access to a GPU everything will go much faster. With an Nvidia RTX2080Ti GPU a full training will take approximately one hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import our module containing helper functions\n",
    "import gravann\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "# pytorch\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib notebook\n",
    "\n",
    "# Ensure that changes in imported module (gravann most importantly) are autoreloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# If possible enable CUDA\n",
    "gravann.enableCUDA()\n",
    "gravann.fixRandomSeeds()\n",
    "device = os.environ[\"TORCH_DEVICE\"]\n",
    "print(\"Will use device \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and visualizing the ground truth asteroid (a point cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the ground truth (a mascon model of some body)\n",
    "with open(\"mascons/Eros.pk\", \"rb\") as file:\n",
    "    mascon_points, mascon_masses, mascon_name = pk.load(file)\n",
    "    \n",
    "mascon_points = torch.tensor(mascon_points)\n",
    "mascon_masses = torch.tensor(mascon_masses)\n",
    "\n",
    "# Print some information on the loaded ground truth \n",
    "# (non-dimensional units assumed. All mascon coordinates are thus in -1,1 and the mass is 1)\n",
    "print(\"Name: \", mascon_name)\n",
    "print(\"Number of mascons: \", len(mascon_points))\n",
    "print(\"Total mass: \", sum(mascon_masses))\n",
    "\n",
    "# Each mascon has a closest neighbour. The least close one is here computed (non dimensional units).\n",
    "# Its a quantity that is of interest when checking mascon models coming from gravitationally stable aggregates.\n",
    "print(\"Maximal minimal distance:\",gravann.max_min_distance(mascon_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we visualize the loaded ground truth\n",
    "gravann.plot_mascon(mascon_points, mascon_masses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing the asteroid mass distribution via a neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Defining the network architecture\n",
    "We here use functions from our module as to not clutter the notebook, but the code is rather straight forward: a FFNN with some options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding: direct encoding (i.e. feeding the network directly with the Cartesian coordinates in the unit hypercube)\n",
    "# was found to work well in most cases. But more options are implemented in the module.\n",
    "encoding = gravann.direct_encoding()\n",
    "\n",
    "# The model is here a SIREN network (FFNN with sin non linearities and a final absolute value to predict the density)\n",
    "model = gravann.init_network(encoding, n_neurons=100, model_type=\"siren\", activation = gravann.AbsLayer())\n",
    "\n",
    "# When a new network is created we init empty training logs\n",
    "loss_log = []\n",
    "weighted_average_log = []\n",
    "running_loss_log = []\n",
    "n_inferences = []\n",
    "# .. and we init a loss trend indicators\n",
    "weighted_average = deque([], maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IF YOU WANT TO LOAD AN ALREADY TRAINED NETWORK UNCOMMENT HERE.\n",
    "## It is important that the network architecture is compatible, otherwise this will fail\n",
    "\n",
    "#model.load_state_dict(torch.load(\"model.mdl\"))\n",
    "\n",
    "# Once a model is loaded the learned constant c (named kappa in the paper) is unknown \n",
    "# and must be relearned (ideally it should also be saved at the end of the training as it is a learned parameter)\n",
    "#c = gravann.compute_c_for_model(model, encoding, mascon_points, mascon_masses, use_acc = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Visualizing the initial neural density field\n",
    "The network output is a mass density in the unit cube. Essentially a three dimensional function which we here plot via rejection sampling (for now this method is good enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rejection plot will visualize the neural density field as a probability distribution function. \n",
    "# Colors represent the density magnitude (white being zero). \n",
    "# At the beginning, the whole hypercube is filled with some non vanishing density.\n",
    "gravann.plot_model_rejection(model, encoding, views_2d=False, N=10000, progressbar=True, c=1)\n",
    "plt.title(\"Believe it or not I am an asteroid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of a geodesyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTAL SETUP ------------------------------------------------------------------------------------\n",
    "# Number of points to be used to evaluate numerically the triple integral\n",
    "# defining the acceleration. \n",
    "# Use <=30000 to for a quick training ... 300000 was used to produce most of the paper results\n",
    "n_quadrature = 300000\n",
    "\n",
    "# Dimension of the batch size, i.e. number of points\n",
    "# where the ground truth is compared to the predicted acceleration\n",
    "# at each training epoch.\n",
    "# Use 100 for a quick training. 1000  was used to produce most of the paper results\n",
    "batch_size = 256\n",
    "\n",
    "# Loss. The normalized L1 loss (kMAE in the paper) was\n",
    "# found to be one of the best performing choices.\n",
    "# More are implemented in the module\n",
    "loss_fn = gravann.normalized_L1_loss\n",
    "\n",
    "# The numerical Integration method. \n",
    "# Trapezoidal integration is here set over a dataset containing acceleration values,\n",
    "# (it is possible to also train on values of the gravity potential, results are similar)\n",
    "integration = gravann.tq_integrate\n",
    "# integration = gravann.ACC_trap\n",
    "\n",
    "# The sampling method to decide what points to consider in each batch.\n",
    "# In this case we sample points unifromly in a sphere and reject those that are inside the asteroid\n",
    "targets_point_sampler = gravann.get_target_point_sampler(batch_size, \n",
    "                                                         limit_shape_to_asteroid=\"3dmeshes/Eros_lp.pk\", \n",
    "                                                         method=\"spherical\", \n",
    "                                                         bounds=[0,1])\n",
    "# Here we set the optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.8, patience = 200, min_lr = 1e-6,verbose=True)\n",
    "\n",
    "# And init the best results\n",
    "best_loss = np.inf\n",
    "best_model_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: the cell below is explicitly typed for convenience, as this is a tutorial-ish after all, but the module gravann contains a function (train_on_batch) that does the same**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRAINING LOOP (normal training, no use of any prior shape information)------------------------\n",
    "# This cell can be stopped and started again without loosing memory of the training nor its logs\n",
    "torch.cuda.empty_cache()\n",
    "# The main training loop\n",
    "for i in range(2000):\n",
    "    # Each ten epochs we resample the target points\n",
    "    if (i % 10 == 0):\n",
    "        target_points = targets_point_sampler()\n",
    "        # We compute the labels whenever the target points are changed\n",
    "        labels = gravann.ACC_L(target_points, mascon_points, mascon_masses)\n",
    "    \n",
    "    # We compute the values predicted by the neural density field\n",
    "    predicted = integration(target_points, model, encoding, N=n_quadrature)\n",
    "    \n",
    "    # We learn the scaling constant (k in the paper)\n",
    "    c = torch.sum(predicted*labels)/torch.sum(predicted*predicted)\n",
    "    \n",
    "    # We compute the loss (note that the contrastive loss needs a different shape for the labels)\n",
    "    if loss_fn == gravann.contrastive_loss:\n",
    "       loss = loss_fn(predicted, labels)\n",
    "    else:\n",
    "       loss = loss_fn(predicted.view(-1), labels.view(-1))\n",
    "    \n",
    "    # We store the model if it has the lowest fitness \n",
    "    # (this is to avoid losing good results during a run that goes wild)\n",
    "    if loss < best_loss:\n",
    "        best_model_state_dict = model.state_dict()\n",
    "        best_loss = loss\n",
    "        print('New Best: ', loss.item())\n",
    "    \n",
    "    # Update the loss trend indicators\n",
    "    weighted_average.append(loss.item())\n",
    "    \n",
    "    # Update the logs\n",
    "    weighted_average_log.append(np.mean(weighted_average))\n",
    "    loss_log.append(loss.item())\n",
    "    n_inferences.append((n_quadrature*batch_size) // 1000)\n",
    "    \n",
    "    # Print every i iterations\n",
    "    if i % 25 == 0:\n",
    "        wa_out = np.mean(weighted_average)\n",
    "        print(f\"It={i}\\t loss={loss.item():.3e}\\t  weighted_average={wa_out:.3e}\\t  c={c:.3e}\")\n",
    "        \n",
    "    # Zeroes the gradient (necessary because of things)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Perform a step in LR scheduler to update LR\n",
    "    scheduler.step(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we restore the learned parameters of the best model of the run\n",
    "for layer in model.state_dict():\n",
    "    model.state_dict()[layer] = best_model_state_dict[layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation of the neural density field learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First lets have a look at the training loss history\n",
    "plt.figure()\n",
    "abscissa = np.cumsum(n_inferences)\n",
    "plt.semilogy(abscissa, loss_log)\n",
    "plt.semilogy(abscissa, weighted_average_log)\n",
    "plt.xlabel(\"Thousands of model evaluations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\",\"Weighted Average Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets have a look at the neural density field.\n",
    "# First with a rejection plot\n",
    "gravann.plot_model_rejection(model, encoding, views_2d=True, bw=True, N=1500, alpha=0.1, s=50, c=c, crop_p=0.1, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Then overlaying a heatmap to the mascons \n",
    "gravann.plot_model_vs_mascon_contours(model, encoding, mascon_points, mascon_masses,c=c, progressbar = True, N=2500, heatmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computes the Validation table with rel and abs errors on the predicted acceleration (w.r.t. ground truth) \n",
    "# at low, med, high altitudes (see paper). is requires sampling quite a lot, so it takes time \n",
    "gravann.validation(model, encoding, mascon_points, mascon_masses, use_acc=True, asteroid_pk_path=\"3dmeshes/Eros.pk\", N=10000, N_integration=300000, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows how the relative error is distribued along the asteroid surface (at some altitude) \n",
    "gravann.plot_model_mascon_acceleration(\"3dmeshes/Eros.pk\", model, encoding, mascon_points, mascon_masses, plane=\"XY\", c=c, N=5000, logscale=False, altitude=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the Contour plot of the gravity potential generated by the neural density field.\n",
    "gravann.plot_potential_contours(model, encoding, mascon_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save the model\n",
    "#torch.save(model.state_dict(), \"model.mdl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
