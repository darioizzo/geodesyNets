{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on data from the Osiris-Rex mission\n",
    "\n",
    "In this notebook we perform the training of a GeodesyNet using the observed trajectories of detached pebbles.\n",
    "\n",
    "During the time of observation, Bennu ejected multiple small rock pebbles. Many of these stayed in orbit around Bennu for several days before either falling back to Bennu's surface or escaping its gravitational influence. These trajectories yield additional samples of the gravity field. However, due to their small size, they have a high surface-to-mass ratio and radiation effects play a large role, adding substantial unmodelled effects to their trajectories and thus to the value of the purely gravitational acceleration that can be computed from them.\n",
    "\n",
    "NOTE: With respect to a normal training (see Starter Notebook) the difference is only on the dataset used. In the Starter Notebook we use synthetically generated data provided via a sampler. Here we use data precomputed from the real observed pebble trajectories. Here we do not make use of any prior knowledge on Bennu shape model.\n",
    "\n",
    "We suggest to run this notebook in the same conda environment as the one described in the Starter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# core stuff\n",
    "import gravann\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "# pytorch\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib notebook\n",
    "\n",
    "# Ensure that changes in imported module (gravann most importantly) are autoreloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# If possible enable CUDA\n",
    "gravann.enableCUDA()\n",
    "gravann.fixRandomSeeds()\n",
    "device = os.environ[\"TORCH_DEVICE\"]\n",
    "print(\"Will use device \",device) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and visualizing the ground truth asteroid (a point cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mascon_points,mascon_masses, mascon_masses_nu = gravann.load_sample(\"Bennu.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravann.plot_mascon(mascon_points, mascon_masses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the trajectory data, created on a different notebook from SPICE kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"osirisrex/bennu_pebbles_filtered.pk\", \"rb\") as file:\n",
    "    _,_,data_points_p, data_labels_p = pk.load(file)\n",
    "data_points_p=torch.tensor(data_points_p)\n",
    "data_labels_p=torch.tensor(data_labels_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing an asteroid via a neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Instantiating the network\n",
    "The networks inputs are the cartesian coordinates of a point in the unit cube, encoded via some transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding choosen\n",
    "encoding = gravann.direct_encoding()\n",
    "\n",
    "# For \"normal\" training\n",
    "model = gravann.init_network(encoding, model_type=\"siren\")\n",
    "\n",
    "# For differnential training\n",
    "# model = gravann.init_network(encoding, model_type=\"siren\", activation = nn.Tanh())\n",
    "\n",
    "# When a new network is created we init empty training logs\n",
    "loss_log = []\n",
    "weighted_average_log = []\n",
    "running_loss_log = []\n",
    "n_inferences = []\n",
    "# .. and we init some loss trend indicators\n",
    "weighted_average = deque([], maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU NOW WANT TO LOAD THE ALREADY TRAINED NETWORK UNCOMMENT HERE.\n",
    "## It is important that the network architecture is compatible, otherwise this will fail\n",
    "#model.load_state_dict(torch.load(\"FILENAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing an asteroid represented by the network\n",
    "The network output is the density in the unit cube. It is, essentially, a three dimensional function. (Does not work with differetial training!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravann.plot_model_rejection(model, encoding, views_2d=False, N=2500, progressbar=True, c=10)\n",
    "plt.title(\"Believe me, I am an asteroid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The ANN to match the ground truth potential\n",
    "\n",
    "Let it run up to when its < 1e-3 to actually see something that resembles the original asteroid. When stuck increase the number of monte carlo samples or play around the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTAL SETUP ------------------------------------------------------------------------------------\n",
    "# Here we set some hyperparameters\n",
    "N_integration = 30000\n",
    "batch_size = 100\n",
    "\n",
    "# Here we set the loss\n",
    "loss_fn = gravann.normalized_L1_loss\n",
    "\n",
    "# Here we set the choosen Integration method\n",
    "integrator = gravann.ACC_trap\n",
    "\n",
    "# Here we set the optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, factor=0.8, patience=200, min_lr=1e-6, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This cell can be stopped and started again without loosing memory of the training nor its indicators\n",
    "torch.cuda.empty_cache()\n",
    "plt.close('all')\n",
    "# The main training loop\n",
    "for i in range(5000):\n",
    "    # Sample random data from our observations,\n",
    "    # This might want to into a separate sampler in\n",
    "    # _sample_observation_points.py and an associated \n",
    "    # labels samplers in masocn_labels.py\n",
    "    idxs = np.random.choice(np.arange(len(data_points_p)), batch_size, replace=False)\n",
    "    target_points = data_points_p[idxs]\n",
    "    labels = data_labels_p[idxs]\n",
    "    \n",
    "    # Compute the loss (use N=3000 to start with, then, eventually, beef it up to 200000)\n",
    "    predicted = integrator(target_points, model, encoding, N=N_integration)\n",
    "    c = torch.sum(predicted*labels)/torch.sum(predicted*predicted)\n",
    "    if loss_fn == gravann.contrastive_loss or loss_fn == gravann.normalized_relative_component_loss:\n",
    "       loss = loss_fn(predicted, labels)\n",
    "    else:\n",
    "       loss = loss_fn(predicted.view(-1), labels.view(-1))\n",
    "    \n",
    "    # Update the loss trend indicators\n",
    "    weighted_average.append(loss.item())\n",
    "    weighted_average_log.append(np.mean(weighted_average))\n",
    "    loss_log.append(loss.item())\n",
    "    n_inferences.append((N_integration*batch_size) // 1000)\n",
    "    \n",
    "    # Print every i iterations\n",
    "    if i % 25 == 0:\n",
    "        wa_out = np.mean(weighted_average)\n",
    "        print(f\"It={i}\\t loss={loss.item():.3e}\\t  weighted_average={wa_out:.3e}\\t  c={c:.3e}\")\n",
    "    \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Perform a step in LR scheduler to update LR\n",
    "    scheduler.step(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rejection plot\n",
    "gravann.plot_model_rejection(model, encoding, views_2d=True, bw=True, N=1500, alpha=0.1, s=50, c=c, crop_p=0.1, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the loss history\n",
    "plt.figure()\n",
    "abscissa = np.cumsum(n_inferences)\n",
    "plt.semilogy(abscissa, loss_log)\n",
    "plt.semilogy(abscissa, weighted_average_log)\n",
    "plt.xlabel(\"Thousands of model evaluations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\",\"Weighted Average Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rejection plot overlayed with the mascon\n",
    "gravann.plot_model_vs_mascon_contours(model, encoding, mascon_points, mascon_masses,c=c, progressbar = True, N=2500, heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute the acceleration plot\n",
    "gravann.plot_model_mascon_acceleration(\"3dmeshes/Bennu.pk\", model, encoding, mascon_points, mascon_masses, plane=\"XY\", c=c, N=5000, logscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravann.plot_model_mascon_acceleration(\"3dmeshes/Bennu.pk\", model, encoding, mascon_points, mascon_masses, plane=\"XZ\", c=c, N=5000, logscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravann.plot_model_mascon_acceleration(\"3dmeshes/Bennu.pk\", model, encoding, mascon_points, mascon_masses, plane=\"YZ\", c=c, N=5000, logscale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save to models\n",
    "#torch.save(model.state_dict(), \"models/siren_acc_bennu.mdl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
