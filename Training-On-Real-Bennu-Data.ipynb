{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an ANN to represent the gravity acceleration of Bennu \n",
    "\n",
    "In this notebook we use the trajectory of OSIRIS-Rex and of the pebbles to reverse the gravity field of Bennu\n",
    "\n",
    "To run this notebook (and the others) create a conda environment using the following commands:\n",
    "```\n",
    " conda create -n geodesyann python=3.8 ipython scikit-learn numpy h5py matplotlib jupyter\n",
    " conda activate geodesyann\n",
    " conda install -c pytorch pytorch\n",
    " conda install -c open3d-admin open3d\n",
    " pip install sobol_seq\n",
    " pip install tetgen\n",
    " conda install pyvista pyvistaqt\n",
    "```\n",
    "\n",
    "To use CUDA, you will need to run\n",
    "```\n",
    " conda install pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# core stuff\n",
    "import gravann\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "# pytorch\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib notebook\n",
    "\n",
    "# Ensure that changes in imported module (gravann most importantly) are autoreloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# If possible enable CUDA\n",
    "gravann.enableCUDA()\n",
    "gravann.fixRandomSeeds()\n",
    "device = os.environ[\"TORCH_DEVICE\"]\n",
    "print(\"Will use device \",device) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and visualizing the ground truth asteroid (a point cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mascons/Bennu_nu.pk\", \"rb\") as file:\n",
    "    mascon_points, mascon_masses, mascon_name = pk.load(file)\n",
    "    \n",
    "mascon_points = torch.tensor(mascon_points)\n",
    "mascon_masses = torch.tensor(mascon_masses)\n",
    "\n",
    "print(\"Name: \", mascon_name)\n",
    "print(\"Number of points: \", len(mascon_points))\n",
    "print(\"Total mass: \", sum(mascon_masses))\n",
    "\n",
    "print(\"Maximal minimal distance:\",gravann.max_min_distance(mascon_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravann.plot_mascon(mascon_points, mascon_masses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the trajectory data, created on a different notebook from SPICE kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"osirisrex/pickled_bennu_900_cleaned.pk\", \"rb\") as file:\n",
    "    data_points, data_labels = pk.load(file)\n",
    "data_points=torch.tensor(data_points)\n",
    "data_labels=torch.tensor(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"osirisrex/bennu_pebbles_filtered.pk\", \"rb\") as file:\n",
    "    _,_,data_points_p, data_labels_p = pk.load(file)\n",
    "data_points_p=torch.tensor(data_points_p)\n",
    "data_labels_p=torch.tensor(data_labels_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing an asteroid via a neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Instantiating the network\n",
    "The networks inputs are the cartesian coordinates of a point in the unit cube, encoded via some transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding choosen\n",
    "encoding = gravann.directional_encoding()\n",
    "model = gravann.init_network(encoding, n_neurons=100, model_type=\"default\")\n",
    "\n",
    "# When a new network is created we init empty training logs\n",
    "loss_log = []\n",
    "weighted_average_log = []\n",
    "running_loss_log = []\n",
    "n_inferences = []\n",
    "# .. and we init some loss trend indicators\n",
    "weighted_average = deque([], maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU NOW WANT TO LOAD THE ALREADY TRAINED NETWORK UNCOMMENT HERE.\n",
    "## It is important that the network architecture is compatible, otherwise this will fail\n",
    "#model.load_state_dict(torch.load(\"FILENAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing an asteroid represented by the network\n",
    "The network output is the density in the unit cube. It is, essentially, a three dimensional function we plot via rejection sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravann.plot_model_rejection(model, encoding, views_2d=False, N=2500, progressbar=True, c=10)\n",
    "plt.title(\"Believe me, I am an asteroid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The ANN to match the ground truth potential\n",
    "\n",
    "Let it run up to when its < 1e-3 to actually see something that resembles the original asteroid. When stuck increase the number of monte carlo samples or play around the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTAL SETUP ------------------------------------------------------------------------------------\n",
    "# Here we set some hyperparameters\n",
    "N_mc_points = 30000\n",
    "batch_size = 100\n",
    "\n",
    "# Here we set the loss\n",
    "#loss_fn = gravann.mse_loss\n",
    "loss_fn = gravann.normalized_L1_loss\n",
    "\n",
    "# Here we set the choosen Integration method\n",
    "#mc_method = gravann.ACC_ld\n",
    "mc_method = gravann.ACC_trap\n",
    "\n",
    "# Here we set the optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.5, patience = 5000, min_lr = 5e-6,verbose=True)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9,nesterov=True)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0005, max_lr=0.5, step_size_up=250, mode=\"exp_range\", gamma = .997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This cell can be stopped and started again without loosing memory of the training nor its indicators\n",
    "torch.cuda.empty_cache()\n",
    "plt.close('all')\n",
    "# The main training loop\n",
    "for i in range(5000):\n",
    "    # Each ten epochs we resample the target points\n",
    "    if (i % 10 == 0):\n",
    "        idxs = np.random.choice(np.arange(len(data_points_p)), batch_size, replace=False)\n",
    "        target_points = data_points_p[idxs]\n",
    "        labels = data_labels_p[idxs]\n",
    "    \n",
    "    # Compute the loss (use N=3000 to start with, then, eventually, beef it up to 200000)\n",
    "    predicted = mc_method(target_points, model, encoding, N=N_mc_points)\n",
    "    c = torch.sum(predicted*labels)/torch.sum(predicted*predicted)\n",
    "    if loss_fn == gravann.contrastive_loss or loss_fn == gravann.normalized_relative_component_loss:\n",
    "       loss = loss_fn(predicted, labels)\n",
    "    else:\n",
    "       loss = loss_fn(predicted.view(-1), labels.view(-1))\n",
    "    # Update the loss trend indicators\n",
    "    weighted_average.append(loss.item())\n",
    "    # Update the logs\n",
    "    weighted_average_log.append(np.mean(weighted_average))\n",
    "    loss_log.append(loss.item())\n",
    "    n_inferences.append((N_mc_points*batch_size) // 1000)\n",
    "    # Print every i iterations\n",
    "    if i % 25 == 0:\n",
    "        wa_out = np.mean(weighted_average)\n",
    "        print(f\"It={i}\\t loss={loss.item():.3e}\\t  weighted_average={wa_out:.3e}\\t  c={c:.3e}\")\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Perform a step in LR scheduler to update LR\n",
    "    scheduler.step(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rejection plot\n",
    "gravann.plot_model_rejection(model, encoding, views_2d=True, bw=True, N=1500, alpha=0.1, s=50, c=c, crop_p=0.1, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the loss history\n",
    "plt.figure()\n",
    "abscissa = np.cumsum(n_inferences)\n",
    "plt.semilogy(abscissa, loss_log)\n",
    "plt.semilogy(abscissa, weighted_average_log)\n",
    "plt.xlabel(\"Thousands of model evaluations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\",\"Weighted Average Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rejection plot overlayed with the mascon\n",
    "gravann.plot_model_vs_mascon_contours(model, encoding, mascon_points, mascon_masses,c=c, progressbar = True, N=2500, heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generating a mesh out of the model\n",
    "gravann.plot_model_mesh(model,encoding,rho_threshold=c.cpu().detach().numpy()*1e-2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute c \n",
    "#target_points = targets_point_sampler()\n",
    "#labels = gravann.ACC_L(target_points, mascon_points, mascon_masses)\n",
    "#predicted = mc_method(target_points, model, encoding, N=N_mc_points)\n",
    "#c = torch.sum(predicted*labels)/torch.sum(predicted*predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the Validation table\n",
    "gravann.validation(model, encoding, mascon_points, mascon_masses, use_acc=True, asteroid_pk_path=\"3dmeshes/Bennu_lp.pk\", N=5000, N_integration=500000, batch_size=32, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute the acceleration plot\n",
    "gravann.plot_model_mascon_acceleration(\"3dmeshes/Bennu.pk\", model, encoding, mascon_points, mascon_masses, plane=\"XY\", c=c, N=5000, logscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the acceleration plot\n",
    "gravann.plot_model_mascon_acceleration(\"3dmeshes/Bennu.pk\", model, encoding, mascon_points, mascon_masses, plane=\"XZ\", c=c, N=5000, logscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravann.plot_model_mascon_acceleration(\"3dmeshes/Bennu.pk\", model, encoding, mascon_points, mascon_masses, plane=\"YZ\", c=c, N=5000, logscale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save to models/cluster_xxxx\n",
    "#torch.save(model.state_dict(), \"models/siren_acc_bennu.mdl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
