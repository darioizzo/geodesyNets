{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling irregular bodies shape via ANNs\n",
    "In this notebook we explore the possibility to use ANNs to represent the generic shape and density of an irregular body and be trained to reproduce a known gravitational potential field.\n",
    "\n",
    "To get statically stable asteroids we use results from MPIA work by Francesco Biscani obtained during simulation of protoplanetary formation made by large n-body simulations. Data are included as a submodule in the git project.\n",
    "\n",
    "To run this notebook create a conda environment using the following commands:\n",
    "```\n",
    " conda create -n geodesyann python=3.8 ipython scikit-learn numpy h5py matplotlib\n",
    " conda install -c open3d-admin open3d\n",
    "```\n",
    "\n",
    "And you will need pytorch (CPU is enough) for the ANN part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "# For debugging and development purposes this is now set to float64 ... change for speed on GPUs\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and visualizing an asteroid as a point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the data from MPIA containing pseudo-stable asteroid shapes\n",
    "f = h5py.File('sample_vis_data/sample_01/state_10567.hdf5','r')\n",
    "f2 = h5py.File('sample_vis_data/sample_01/global.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file state_ ... contains the positions of all particles as well as the indices\n",
    "# of those belonging to a cluster. Here we extract the largest ones.\n",
    "dims = [(len(f[cluster][()]), cluster) for cluster in f.keys() if 'cluster' in cluster]\n",
    "largest_clusters = sorted(dims,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have ordered the largest asteroids, we now extract positions for one in particular\n",
    "rank = 4\n",
    "print(\"Target: \", largest_clusters[rank][1])\n",
    "# The particles idxs for this cluster\n",
    "idx = f[largest_clusters[rank][1]][()]\n",
    "# The particle radius\n",
    "radius = f2['radius'][()]\n",
    "# Particle positions\n",
    "x_raw = f['x'][()][idx]\n",
    "y_raw = f['y'][()][idx]\n",
    "z_raw = f['z'][()][idx]\n",
    "print(\"Diameter: \", 2 * radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "# We put xyz in a different shape (point_cloud)\n",
    "point_cloud = np.append(x_raw, np.append(y_raw,z_raw))\n",
    "point_cloud = point_cloud.reshape((3,len(x_raw)))\n",
    "point_cloud = np.transpose(point_cloud)\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=4, algorithm='ball_tree').fit(point_cloud)\n",
    "distances, indices = nbrs.kneighbors(point_cloud)\n",
    "\n",
    "print(\"Minimum distance between particles: \", min(distances[:,1]))\n",
    "print(\"Maximum distance between particles: \", max(distances[:,1]))\n",
    "\n",
    "# We take out particles that are not \"touching\" at least two neighbours\n",
    "unstable_points = np.where(distances[:, 3]> 2 * radius * 1.01)[0]\n",
    "print(\"Number of unstable points: \", len(unstable_points))\n",
    "x = np.delete(x_raw, unstable_points, 0)\n",
    "y = np.delete(y_raw, unstable_points, 0)\n",
    "z = np.delete(z_raw, unstable_points, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We subtract the mean so that the origin is the center of figure\n",
    "x = x - np.mean(x)\n",
    "y = y - np.mean(y)\n",
    "z = z - np.mean(z)\n",
    "# We normalize so that the axes are at most one\n",
    "max_value = max([max(abs(it)) for it in [x,y,z]])\n",
    "x = x / max_value\n",
    "y = y / max_value\n",
    "z = z / max_value\n",
    "plot_radius = radius /  max_value  * 3000\n",
    "# We put xyz in a different shape (point_cloud)\n",
    "point_cloud = np.append(x, np.append(y,z))\n",
    "point_cloud = point_cloud.reshape((3,len(x)))\n",
    "point_cloud = np.transpose(point_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization via matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "\n",
    "# We color the points w.r.t. their distance to the center\n",
    "color = [np.linalg.norm([point]) for point in point_cloud] \n",
    "# And visualize the masses\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x, y, z, color = 'k', s = plot_radius/2, alpha=0.5)\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "ax.set_zlim([-1,1])\n",
    "ax.view_init(elev=45., azim=125.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization via open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "colors = np.exp(-np.array(color))\n",
    "colors = np.append(np.append(colors, colors), colors).reshape((3, len(color))).transpose()\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "\n",
    "hull, _ = pcd.compute_convex_hull()\n",
    "hull_ls = o3d.geometry.LineSet.create_from_triangle_mesh(hull)\n",
    "hull_ls.paint_uniform_color((1, 0, 0))\n",
    "o3d.visualization.draw_geometries([pcd, hull_ls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the gravitational potential of an asteroid (point cloud) in a point \n",
    "The Canvendish constant is not included (or $G=1$), so that we have:\n",
    "$$\n",
    "U_L = - \\sum_{i=1}^N \\frac{m_i}{|\\mathbf x - \\mathbf r_i|}\n",
    "$$\n",
    "where, assuming the asteroid with a unitary mass $m_i = 1/N$, hence:\n",
    "$$\n",
    "U_L = - \\frac 1N \\sum_{i=1}^N \\frac{1}{|\\mathbf x - \\mathbf r_i|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U_L(r, point_cloud):\n",
    "    retval=torch.empty(len(r),1)\n",
    "    # Only for the points inside we accumulate the integrand (MC method)\n",
    "    for i, radius in enumerate(r):\n",
    "        retval[i] = torch.mean(1./torch.norm(torch.sub(point_cloud,radius), dim=1))\n",
    "    return - retval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp = torch.tensor(point_cloud)\n",
    "r = torch.rand(100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit U_L(r, point_cloud=tcp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing the asteroid via a neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach N.1: A FFNN represents the asteroid surface\n",
    "In this approach we try to represent the asteroid minimalistically via its surface. Assuming an internal uniform density we will then be able to compute the potential generated at some point.\n",
    "\n",
    "The FFNN will get three inputs (expressing in a continuous fashion a direction as a unit vector) and output the distance of the asteroid surface from the origin in that particular direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.uniform_(m.weight.data, -1, 1)\n",
    "        nn.init.uniform_(m.bias.data, -0.1, 0.1)\n",
    "\n",
    "# Initializing a FFNN with three inputs (cartesian components of the unit direction vector) \n",
    "# and one output (the asteroid radius)\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(3,10),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(10,10),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(10,10),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(10,1),\n",
    "          nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "# Applying our weight initialization\n",
    "_ = model.apply(weights_init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating N equally spaced unit vectors on a sphere (to then plot r)\n",
    "N = 40\n",
    "inputs = []\n",
    "u, v = torch.meshgrid((torch.linspace(0., 1., N), torch.linspace(0, 1, N)))\n",
    "theta, phi = 2 * np.pi * u, torch.acos(2.*v-1.)\n",
    "x = torch.sin(phi)*torch.cos(theta)\n",
    "y = torch.sin(phi)*torch.sin(theta)\n",
    "z = torch.cos(phi)\n",
    "inputs = torch.cat((x.view(-1,1),y.view(-1,1), z.view(-1,1)), dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting no gradients are needed\n",
    "r = model(inputs).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the visual representation of the asteroid\n",
    "THETA, PHI = theta, phi\n",
    "R = r.view((N,N))\n",
    "\n",
    "X = R * torch.sin(PHI) * torch.cos(THETA)\n",
    "Y = R * torch.sin(PHI) * torch.sin(THETA)\n",
    "Z = R * torch.cos(PHI)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "plot = ax.plot_surface(\n",
    "    X.numpy(), Y.numpy(), Z.numpy(), rstride=1, cstride=1, cmap=plt.get_cmap('jet'),\n",
    "    linewidth=0, antialiased=False, alpha=0.5)\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "ax.set_zlim([-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the gravitational potential of the asteroid ANN model\n",
    "\n",
    "We have a representation of the asteroid surface (and hence volume) given by our ANN, we may then assume a uniform internal density and  compute the resulting potential at a given point $\\mathbf x$ as:\n",
    "\n",
    "$$\n",
    "U_P = \\int_V \\rho \\frac1{|\\mathbf r - \\mathbf x|} dV = \\rho \\int_V \\frac{dV}{|\\mathbf r - \\mathbf x|} \n",
    "$$\n",
    "\n",
    "to compute the integral we use Monte Carlo integration methods, that is we sample N points in a volume V = [-1,1]x[-1,1]x[-1,1] that we know contains our asteroid (as ensured by the sigmoid activation function of the last layer). We then approximate:\n",
    "\n",
    "$$\n",
    "U_P(\\mathbf x) = \\rho\\frac V N  \\sum_i^N \\left\\{\\begin{array}{ll}\\frac{1}{|\\mathbf r_i - \\mathbf x|} & \\mbox{inside the asteroid} \\\\ 0 &  \\mbox{otherwise}  \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Since $\\rho = \\frac MV$ is itself an integral being $V = \\int_V dV \\approx \\frac N{N_{in}V}$ we get:\n",
    "\n",
    "$$\n",
    "U_P(\\mathbf x) = \\frac 1{N_{in}}  \\sum_i^N \\left\\{\\begin{array}{ll}\\frac{1}{|\\mathbf r_i - \\mathbf x|} & \\mbox{inside the asteroid} \\\\ 0 &  \\mbox{otherwise}  \\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U_P(r, model, N = 300):\n",
    "    # We generate randomly 300 points in the [-1,1]^3 bounds\n",
    "    points = torch.rand(N,3) * 2 - 1\n",
    "    # We compute their norms\n",
    "    norms = torch.norm(points, dim=1)\n",
    "    # We compute if they are inside or outside the asteroid represented by the FFNN model\n",
    "    inside = norms.view(-1,1) < model(points/norms.view(-1,1))\n",
    "    # Only for the points inside we accumulate the integrand (MC method)\n",
    "    retval = torch.sum(1./torch.norm(r - points[inside.view(-1,)], dim=1))\n",
    "    return  - retval / torch.sum(inside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class to represent a sphere model\n",
    "class sphere:\n",
    "    def __init__(self, N = 300):\n",
    "        self.N = N\n",
    "    def __call__(self, x):\n",
    "        return torch.ones(self.N,1)\n",
    "\n",
    "spherical_asteroid = sphere(N = 3000)\n",
    "NN_asteroid = model\n",
    "\n",
    "target_r = torch.tensor([[1.,1.,1.]])\n",
    "\n",
    "# We print the values of the potential in r of a sphere, of the ground truth asteroid (point cloud)\n",
    "# and of the asteroid as represented by the NN\n",
    "print(\"Potential of the asteroid represented by the NN (MC): \", U_P(target_r, NN_asteroid, N=3000).item())\n",
    "print(\"Potential of a spherical asteroid (MC): \", U_P(target_r, spherical_asteroid, N=3000).item())\n",
    "print(\"Potential of a spherical asteroid (Real): \", (-1./torch.norm(target_r)).item())\n",
    "print(\"Potential of the point cloud: (Real)\", U_L(target_r, point_cloud=tcp)[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All good, but can we get the gradient of the above potential w.r.t. the weights? -> NOPE\n",
    "# So the only way to evolve this network is EVOLUTION!!!\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://insight-quality.com/wp-content/uploads/2018/04/problem.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach N.2: A FFNN represents the asteroid density\n",
    "In this approach we try to represent the asteroid density $\\rho$ directly! This may seem worse\n",
    "but, in terms of gradients, leads to a more learnable problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.uniform_(m.weight.data, -1, 1)\n",
    "        nn.init.uniform_(m.bias.data, -0.1, 0.1)\n",
    "\n",
    "# Initializing a FFNN with four inputs and one output (the density).\n",
    "# The four inputs are ix, iy, iz, r corresponding to the cartesian point P = r [ix, iy, iz]\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(4,30),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(30,30),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(30,30),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(30,30),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(30,10),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(10,1),\n",
    "          nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "# Applying our weight initialization\n",
    "_  = model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a cartesian position x,y,z into a network input ix, iy, iz, r\n",
    "def cart2inputs(x):\n",
    "    unit_x = x / torch.norm(x,dim=1).view(-1,1)\n",
    "    return torch.cat((unit_x, torch.norm(x,dim=1).view(-1,1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize the network output in the box [-1,1]^3 we compute the NN output on a grid \n",
    "x = torch.linspace(-1,1,20)\n",
    "y = torch.linspace(-1,1,20)\n",
    "z = torch.linspace(-1,1,20)\n",
    "X, Y, Z = torch.meshgrid((x,y,z))\n",
    "nn_inputs = torch.cat((X.reshape(-1,1), Y.reshape(-1,1), Z.reshape(-1,1)), dim=1)\n",
    "nn_inputs = cart2inputs(nn_inputs)\n",
    "RHO = model(nn_inputs).detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot it\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X.reshape(-1,1), Y.reshape(-1,1), Z.reshape(-1,1), marker='.', c=RHO, s=100, alpha=0.2)\n",
    "plt.title(\"Believe me I am an asteoroid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the gravitational potential of the asteroid ANN model\n",
    "Having the mass density $\\rho$ we may now compute the potential field via a monte carlo integration approach.\n",
    "In the naive implementation, we will sample at random $N$ points to compute:\n",
    "$$\n",
    "U_P = - \\int_V \\frac\\rho r dV \n",
    "$$\n",
    "The volume V is the cube $[-1,1]^3$, with a volume of 8. We thus approximate the above integral as:\n",
    "$$\n",
    "U_P(\\mathbf r) \\approx \\frac 8N \\sum_i \\frac {\\rho_i}{|\\mathbf r-\\mathbf x_i|} \n",
    "$$\n",
    "where $\\mathbf x_i$ are $N$ uniformly randomly sampled within the cube -> PLEASE IMPROVE ME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the potential at the points r (e.g. torch.tensor([[np.sqrt(2),np.sqrt(2),0], [10,0,0]]))\n",
    "# from the model\n",
    "def U_P2(r, model, N = 3000):\n",
    "    # We generate randomly points in the [-1,1]^3 bounds\n",
    "    mc_points = torch.rand(N,3) * 2 - 1\n",
    "    nn_inputs = cart2inputs(mc_points)\n",
    "    rho = model(nn_inputs)\n",
    "    retval=torch.empty(len(r),1)\n",
    "    # Only for the points inside we accumulate the integrand (MC method)\n",
    "    for i, radius in enumerate(r):\n",
    "        retval[i] = torch.sum(rho/torch.norm(radius - mc_points, dim=1).view(-1,1)) / N\n",
    "    return  - 8 * retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chain(chain,values):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    x,y,z,c = [],[],[],[]\n",
    "    for i,t in enumerate(chain):\n",
    "        t = t.squeeze()\n",
    "        x.append(t[0].item())\n",
    "        y.append(t[1].item())\n",
    "        z.append(t[2].item())\n",
    "        c.append(values[i].item())\n",
    "    ax.scatter3D(x,y,z,c=c,s=1)\n",
    "    ax.set_xlim((-1.25,1.25))\n",
    "    ax.set_ylim((-1.25,1.25))\n",
    "    ax.set_zlim((-1.25,1.25))\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "\n",
    "def check_boundaries(points,values):\n",
    "    r1 = torch.add(points[:,0] > 1.0,points[:,0] < -1.0)\n",
    "    r2 = torch.add(points[:,1] > 1.0,points[:,1] < -1.0)\n",
    "    r3 = torch.add(points[:,2] > 1.0,points[:,2] < -1.0)\n",
    "    r = r1 + r2\n",
    "    r += r3\n",
    "    values[r] = 0.0\n",
    "#     for i,point in enumerate(points):\n",
    "#         if torch.any(point > 1.0) or torch.any(point < -1.0):\n",
    "#             values[i] = 0.0\n",
    "    return values\n",
    "        \n",
    "    \n",
    "def uniform_prior(N = 1):\n",
    "    return torch.rand(N,3) * 2 - 1\n",
    "\n",
    "def sampled_prior(rho_model,stepsize = 0.05,N_samples = 10000):\n",
    "    #sample rho\n",
    "    sample_points = uniform_prior(N_samples) #pick sample points\n",
    "    nn_inputs = cart2inputs(sample_points) #compute transform \n",
    "    rho_values = rho_model(nn_inputs).squeeze() #compute rhos at points\n",
    "    rho_values = rho_values / torch.norm(rho_values) #normalize values\n",
    "    \n",
    "    #find mean and var\n",
    "    mean = torch.mv(sample_points.t(),rho_values) / N_samples #compute arithmetic mean\n",
    "    print(mean)\n",
    "    variance = torch.sum(((mean - sample_points) * (mean - sample_points)).t()*rho_values,dim=1) \n",
    "    \n",
    "    #define gaussian with that\n",
    "    dist = torch.distributions.MultivariateNormal(mean,stepsize * torch.diag(variance))\n",
    "#     dist = torch.distributions.Uniform(torch.ones(3) * - 1,torch.ones(3))\n",
    "    return dist\n",
    "\n",
    "def U_P3(target_points, model, N = 3000,plot=False,verbose=False, stepsize = 0.05):\n",
    "    prior = sampled_prior(model,stepsize = stepsize)\n",
    "    sample_points = prior.sample([N])\n",
    "    p = torch.exp(prior.log_prob(sample_points))\n",
    "#     p = torch.ones(N) * 0.5 # TODO REMOVE THIS IF NOT USING UNIFORM PRIOR\n",
    "    retval = torch.zeros(len(target_points),1)\n",
    "    nn_inputs = cart2inputs(sample_points)\n",
    "    rho = model(nn_inputs)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Sample points \\n\",sample_points)\n",
    "        print(\"p \\n\",p)\n",
    "        print(\"rho \\n\",rho)\n",
    "    \n",
    "    for i, target_point in enumerate(target_points):\n",
    "        \n",
    "        f_values = rho/torch.norm(target_point - sample_points, dim=1).view(-1,1)\n",
    "        f_values = check_boundaries(sample_points,f_values)\n",
    "        retval[i] = torch.sum(torch.div(f_values.squeeze(),p.squeeze())) / N\n",
    "        \n",
    "        if verbose:\n",
    "            divs = torch.div(f_values,p).detach().numpy()\n",
    "            fs = f_values.detach().numpy()\n",
    "            pv = p.detach().numpy()\n",
    "            print(\"Ps \\t\\t Fs \\t\\t Fs/Ps\")\n",
    "            for p_val,f_val in zip(p,f_values):\n",
    "                print(f\"{p_val.item():.8f},\\t{f_val.item():.8f},\\t{f_val.item()/p_val.item():.8f}\")\n",
    "            print(\"Sum of all fs\",torch.sum(f_values).item())\n",
    "            print(\"Sum of all ps\",torch.sum(p).item())\n",
    "            print(\"Divisors in torch:\",torch.div(f_values.squeeze(),p.squeeze()))\n",
    "            print(\"Sum of fs/ps\",torch.sum(torch.div(f_values.squeeze(),p.squeeze())).item())\n",
    "            print(\"N\",N)\n",
    "            print(\"Integral = \",retval[i].item())\n",
    "        if plot:\n",
    "            plot_chain(sample_points,f_values)\n",
    "    return  -retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate = U_P3(torch.tensor([[2,2,2]]), model, N=10000, plot=True,verbose=False,stepsize=0.05).item()\n",
    "print(estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def show_result(method,result,baseline,N,start,end):\n",
    "    t = end - start\n",
    "    err = torch.mean(torch.abs(result-baseline))\n",
    "    if len(result) > 1:\n",
    "        print(f\"Potential ({method:11s}):  err={err:.8f},{N:10d} steps,   t={t:4f}\")\n",
    "    else:\n",
    "        print(f\"Potential ({method:11s}): {result.item():.8f},   err={err.item():.8f},{N:10d} steps,   t={t:4f}\")\n",
    "\n",
    "points = 2*uniform_prior(256)\n",
    "    \n",
    "N = 1000000\n",
    "start = time.time()\n",
    "baseline = U_P2(points, model, N=N)\n",
    "end = time.time()\n",
    "show_result(\"MC Baseline\",baseline,baseline,N,start,end)\n",
    "print(\"\\n--------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for N in range(1000,52000,5000):\n",
    "for N in range(100,10000,1000):\n",
    "    start = time.time()\n",
    "    vanilla_mc = U_P2(points, model, N=N)\n",
    "    end = time.time()\n",
    "    show_result(\"Vanilla MC\",vanilla_mc,baseline,N,start,end)\n",
    "    \n",
    "print(\"\\n--------------------------------------------------------\\n\")\n",
    "\n",
    "# for N in range(1000,52000,5000):\n",
    "for N in range(100,10000,1000):\n",
    "    start = time.time()\n",
    "    estimate = U_P3(points, model, N=N, stepsize=0.05)\n",
    "    end = time.time()\n",
    "    show_result(\"MC Gaussian\",estimate,baseline,N,start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is here produced. It consists on the values of the potentials in 100 points radnomly\n",
    "# sampled around the asteroid and excluding those that are potentially inside.\n",
    "targets = (torch.rand(100,3)*2-1)*1.1\n",
    "targets[torch.norm(targets, dim=1) > 1]\n",
    "labels = U_L(targets, tcp)\n",
    "\n",
    "# Here we set some details of the training\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we perform steps of gradient descent\n",
    "Let it run up to when its < 1e-3 to actually see something that resembles the original asteroid. When stuck increase the number of monte carlo samples or play around the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5000):\n",
    "    # Compute the loss\n",
    "    predicted = U_P2(targets, model, N=30000)\n",
    "    loss = loss_fn(predicted, labels)\n",
    "    print(i, loss.item())\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1,1,30)\n",
    "y = torch.linspace(-1,1,30)\n",
    "z = torch.linspace(-1,1,30)\n",
    "X, Y, Z = torch.meshgrid((x,y,z))\n",
    "nn_inputs = torch.cat((X.reshape(-1,1), Y.reshape(-1,1), Z.reshape(-1,1)), dim=1)\n",
    "nn_inputs = cart2inputs(nn_inputs)\n",
    "RHO = model(nn_inputs).detach()\n",
    "\n",
    "colors = torch.cat((1-RHO, 1-RHO, 1-RHO, RHO), dim=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X.reshape(-1,1), Y.reshape(-1,1), Z.reshape(-1,1), marker='.', c=colors, s=150)\n",
    "#ax.scatter(targets[:,0], targets[:,1], targets[:,2], color='y')\n",
    "plt.title(\"Believe me I am an asteoroid!\")\n",
    "\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "ax.set_zlim([-1,1])\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "#ax.view_init(elev=45., azim=125.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot((1-RHO).numpy(), '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO list:\n",
    "\n",
    "* Code efficiency -> move to GPU and make training scalable to more sample points / mc points.\n",
    "* MC integration -> a better algorithm must exist to sample the points (low-discrepancy maybe?)\n",
    "* Network architecture -> this is only a possible one. More depth? Different inputs? Use the NERF decomposition in harmonics?\n",
    "* How to visualize and interpret the results? The matplotlib trick used here should be improved? How do we know how well the asteroid shape and internal mass distributionis learned?\n",
    "* Propagate trajectories around the asteroid.\n",
    "* Can we incorporate observations from real spacecraft trajectories in the loss? Or other data-fusion things?\n",
    "* Training with gravity rather than potential?\n",
    "* What happens for non uniform bodies? Can we prove that we are able to follow the inhomogenuities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
