{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an ANN to represent the gravity acceleration produced by a mascon\n",
    "In this notebook we explore the possibility to use ANNs to represent the generic shape and density of an irregular body represented by a mascon model. \n",
    "\n",
    "The loss is defined on the prediction accuracy of the gravitational acceleration field.\n",
    "\n",
    "To run this notebook (and the others) create a conda environment using the following commands:\n",
    "```\n",
    " conda create -n geodesyann python=3.8 ipython scikit-learn numpy h5py matplotlib jupyter\n",
    " conda activate geodesyann\n",
    " conda install -c pytorch pytorch\n",
    " conda install -c open3d-admin open3d\n",
    " pip install sobol_seq\n",
    " pip install tetgen\n",
    " conda install pyvista pyvistaqt\n",
    "```\n",
    "\n",
    "To use CUDA, you will need to run\n",
    "```\n",
    " conda install pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# core stuff\n",
    "import gravann\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "# pytorch\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib notebook\n",
    "\n",
    "# Ensure that changes in imported module (gravann most importantly) are autoreloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# If possible enable CUDA\n",
    "gravann.enableCUDA()\n",
    "gravann.fixRandomSeeds()\n",
    "device = os.environ[\"TORCH_DEVICE\"]\n",
    "print(\"Will use device \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and visualizing the ground truth asteroid (a point cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mascons/Churyumov-Gerasimenko.pk\", \"rb\") as file:\n",
    "    mascon_points, mascon_masses, mascon_name = pk.load(file)\n",
    "    \n",
    "mascon_points = torch.tensor(mascon_points)\n",
    "mascon_masses = torch.tensor(mascon_masses)\n",
    "\n",
    "print(\"Name: \", mascon_name)\n",
    "print(\"Number of points: \", len(mascon_points))\n",
    "print(\"Total mass: \", sum(mascon_masses))\n",
    "\n",
    "print(\"Maximal minimal distance:\",gravann.max_min_distance(mascon_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravann.plot_mascon(mascon_points, mascon_masses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mesh = gravann.plot_point_cloud_mesh(mascon_points, use_top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing an asteroid via a neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Instantiating the network\n",
    "The networks inputs are the cartesian coordinates of a point in the unit cube, encoded via some transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding choosen\n",
    "encoding = gravann.direct_encoding()\n",
    "model = gravann.init_network(encoding, n_neurons=100, model_type=\"default\")\n",
    "\n",
    "# When a new network is created we init empty training logs\n",
    "loss_log = []\n",
    "weighted_average_log = []\n",
    "running_loss_log = []\n",
    "n_inferences = []\n",
    "# .. and we init some loss trend indicators\n",
    "weighted_average = deque([], maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU NOW WANT TO LOAD THE ALREADY TRAINED NETWORK UNCOMMENT HERE.\n",
    "## It is important that the network architecture is compatible, otherwise this will fail\n",
    "#model.load_state_dict(torch.load(\"results/run_02_11_2020/Churyumov-Gerasimenko.pk/LR=0.0001_loss=mse_loss_encoding=direct_encoding_batch_size=1000_target_sample=cubical_activation=Sigmoid/model.mdl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing an asteroid represented by the network\n",
    "The network output is the density in the unit cube. It is, essentially, a three dimensional function and as such it is difficult to plot. \n",
    "\n",
    "### Approach 1: plotting a grid of points colored with the rho value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gravann.plot_model_grid(model, encoding, views_2d=False)\n",
    "plt.title(\"Believe it or not I am an asteroid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: considering rho as a probability density function and sampling points from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravann.plot_model_rejection(model, encoding, views_2d=False, N=2500, progressbar=True, c=1)\n",
    "plt.title(\"Believe it or not I am an asteroid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The ANN to match the ground truth potential\n",
    "\n",
    "Let it run up to when its < 1e-3 to actually see something that resembles the original asteroid. When stuck increase the number of monte carlo samples or play around the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTAL SETUP ------------------------------------------------------------------------------------\n",
    "# Here we set some hyperparameters\n",
    "N_mc_points = 400000\n",
    "batch_size = 1000\n",
    "\n",
    "# Here we set the loss\n",
    "#loss_fn = gravann.mse_loss\n",
    "loss_fn = gravann.contrastive_loss\n",
    "\n",
    "# Here we set the choosen Integration method\n",
    "#mc_method = gravann.ACC_ld\n",
    "mc_method = gravann.ACC_trap\n",
    "\n",
    "# Here we set the method to sample the target points\n",
    "targets_point_sampler = gravann.get_target_point_sampler(batch_size, \n",
    "                                                         limit_shape_to_asteroid=\"3dmeshes/Churyumov-Gerasimenko.pk\", \n",
    "                                                         method=\"spherical\", \n",
    "                                                         bounds=[0,1])\n",
    "# Here we set the optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.5, patience = 5000, min_lr = 5e-6,verbose=True)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9,nesterov=True)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0005, max_lr=0.5, step_size_up=250, mode=\"exp_range\", gamma = .997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell can be stopped and started again without loosing memory of the training nor its indicators\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# The main training loop\n",
    "for i in range(1000):\n",
    "    # Each ten epochs we resample the target points\n",
    "    if (i % 10 == 0):\n",
    "        target_points = targets_point_sampler()\n",
    "    labels = gravann.ACC_L(target_points, mascon_points, mascon_masses)\n",
    "    \n",
    "    # Compute the loss (use N=3000 to start with, then, eventually, beef it up to 200000)\n",
    "    predicted = mc_method(target_points, model, encoding, N=N_mc_points)\n",
    "    c = torch.sum(predicted*labels)/torch.sum(predicted*predicted)\n",
    "    if loss_fn == gravann.contrastive_loss:\n",
    "       loss = loss_fn(predicted, labels)\n",
    "    else:\n",
    "       loss = loss_fn(predicted.view(-1), labels.view(-1))\n",
    "    # Update the loss trend indicators\n",
    "    weighted_average.append(loss.item())\n",
    "    # Update the logs\n",
    "    weighted_average_log.append(np.mean(weighted_average))\n",
    "    loss_log.append(loss.item())\n",
    "    n_inferences.append((N_mc_points*batch_size) // 1000)\n",
    "    # Print every i iterations\n",
    "    if i % 25 == 0:\n",
    "        wa_out = np.mean(weighted_average)\n",
    "        print(f\"It={i}\\t loss={loss.item():.3e}\\t  weighted_average={wa_out:.3e}\\t  c={c:.3e}\")\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Perform a step in LR scheduler to update LR\n",
    "    scheduler.step(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rejection plot\n",
    "gravann.plot_model_rejection(model, encoding, views_2d=True, bw=True, N=1500, alpha=0.1, s=50, c=c, crop_p=0.1, progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the loss history\n",
    "plt.figure()\n",
    "abscissa = np.cumsum(n_inferences)\n",
    "plt.semilogy(abscissa, loss_log)\n",
    "plt.semilogy(abscissa, weighted_average_log)\n",
    "plt.xlabel(\"Thousands of model evaluations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\",\"Weighted Average Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rejection plot overlayed with the mascon\n",
    "gravann.plot_model_vs_mascon_contours(model, encoding, mascon_points, mascon_masses,c=c, progressbar = True, N=2500, levels=[0.01,0.1, 0.2,0.3,0.4,0.5,0.6, 0.7,0.8,0.9,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generating a mesh out of the model\n",
    "gravann.plot_model_mesh(model,encoding,rho_threshold=c.cpu().detach().numpy()*1e-2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = torch.meshgrid(torch.linspace(-1,1,10), torch.linspace(-1,1,10), torch.linspace(-1,1,10))\n",
    "x = x.reshape((-1,1))\n",
    "y = y.reshape((-1,1))\n",
    "z = z.reshape((-1,1))\n",
    "grid = torch.cat((x,y,z), axis=1)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(model(grid)*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = gravann.ACC_L(target_points, mascon_points, mascon_masses)\n",
    "    \n",
    "# Compute the loss (use N=3000 to start with, then, eventually, beef it up to 200000)\n",
    "predicted = mc_method(target_points, model, encoding, N=N_mc_points)\n",
    "c = torch.sum(predicted*labels)/torch.sum(predicted*predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save to models/cluster_xxxx\n",
    "#torch.save(model.state_dict(), \"models/\" + name + \"_\" + encoding.name + \"_ACC.mdl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO list:\n",
    "\n",
    "* Code efficiency -> move to GPU and make training scalable to more sample points / mc points.\n",
    "* MC integration -> importance sampling maybe?\n",
    "* Network architecture -> study different encodings\n",
    "* How to visualize and interpret the results quantitatively.\n",
    "* Propagate trajectories around the asteroids (ground truth and trained).\n",
    "* Incorporate visual cues.\n",
    "* Training with gravity rather than potential?\n",
    "* What happens for non uniform bodies?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
