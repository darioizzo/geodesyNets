{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an ANN to represent the gravity produced by a mascon\n",
    "In this notebook we explore the possibility to use ANNs to represent the generic shape and density of an irregular body represented by a mascon model. The error is defined on the gravitational potential field.\n",
    "\n",
    "A mascon model is a tuple (points, masses, name) containing the x,y,z position of N points in the unit cube and their masses and the name of the mascon.\n",
    "\n",
    "To run this notebook (and the others) create a conda environment using the following commands:\n",
    "```\n",
    " conda create -n geodesyann python=3.8 ipython scikit-learn numpy h5py matplotlib jupyter\n",
    " conda activate geodesyann\n",
    " conda install -c pytorch pytorch\n",
    " conda install -c open3d-admin open3d\n",
    " pip install sobol_seq\n",
    " pip install tetgen\n",
    " conda install pyvista pyvistaqt\n",
    "```\n",
    "\n",
    "To use CUDA, you will need to run\n",
    "```\n",
    " conda install pytorch torchvision cudatoolkit=10.2 -c pytorch\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# core stuff\n",
    "import gravann\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os\n",
    "\n",
    "# pytorch\n",
    "from torch import nn\n",
    "import torch\n",
    "# For debugging and development purposes this is now set to float64 ... change for speed on GPUs\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "# plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib notebook\n",
    "\n",
    "# Ensure that changes in imported module (gravann most importantly) are autoreloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# If possible enable CUDA\n",
    "gravann.enableCUDA()\n",
    "device = os.environ[\"TORCH_DEVICE\"]\n",
    "print(\"Will use device \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and visualizing the ground truth asteroid (a point cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mascons/sample_01_cluster_2400.pk\", \"rb\") as file:\n",
    "    points, masses, name = pk.load(file)\n",
    "    \n",
    "points = torch.tensor(points)\n",
    "masses = torch.tensor(masses)\n",
    "\n",
    "print(\"Name: \", name)\n",
    "print(\"Number of points: \", len(points))\n",
    "print(\"Total mass: \", sum(masses))\n",
    "\n",
    "print(\"Maximal minimal distance:\",gravann.max_min_distance(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravann.plot_mascon(points, masses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mesh = gravann.plot_point_cloud_mesh(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing an asteroid via a neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Instantiating the network\n",
    "The networks inputs are the cartesian coordinates of a point in the unit cube, encoded via some transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding choosen\n",
    "encoding = gravann.directional_encoding()\n",
    "\n",
    "# Network initialization scheme (note that if xavier uniform is used all outputs will start at, roughly 0.5)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.uniform_(m.bias.data, -0.0, 0.0)\n",
    "\n",
    "# Network architecture. Note that the dimensionality of the first linear layer must match the output\n",
    "# of the encoding chosen\n",
    "n_neurons = 100\n",
    "model = nn.Sequential(\n",
    "          nn.Linear(encoding.dim,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,n_neurons),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_neurons,1),\n",
    "          nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "\n",
    "# Applying our weight initialization\n",
    "_  = model.apply(weights_init)\n",
    "\n",
    "# When a new network is created we init empty training logs\n",
    "loss_log = []\n",
    "weighted_average_log = []\n",
    "running_loss_log = []\n",
    "n_inferences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU NOW WANT TO LOAD THE ALREADY TRAINED NETWORK UNCOMMENT HERE.\n",
    "## It is important that the network architecture is compatible, otherwise this will fail\n",
    "#model.load_state_dict(torch.load(\"models/\" + name + \"_\" + encoding.name + \".mdl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing an asteroid represented by the network\n",
    "The network output is the density in the unit cube. It is, essentially, a three dimensional function and as such it is difficult to plot. \n",
    "\n",
    "### Approach 1: plotting a grid of points colored with the rho value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravann.plot_model_grid(model, encoding, views_2d=False)\n",
    "plt.title(\"Believe it or not I am an asteroid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: considering rho as a probability density function and sampling points from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravann.plot_model_rejection(model, encoding, views_2d=False)\n",
    "plt.title(\"Believe it or not I am an asteroid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The ANN to match the ground truth potential\n",
    "\n",
    "Let it run up to when its < 1e-3 to actually see something that resembles the original asteroid. When stuck increase the number of monte carlo samples or play around the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the loss\n",
    "loss_fn = gravann.normalized_loss\n",
    "\n",
    "# Here we set the choosen Integration method\n",
    "mc_method = gravann.U_Pld\n",
    "#mc_method = gravann.U_trap_opt\n",
    "\n",
    "# Here we set the optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor = 0.5, patience = 2500, min_lr = 5e-6,verbose=True)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9,nesterov=True)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0005, max_lr=0.5, step_size_up=250, mode=\"exp_range\", gamma = .997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell can be stopped and started again without loosing memory\n",
    "# of the training nor its indicators\n",
    "N_mc_points = 3000\n",
    "batch_size = 100\n",
    "# Init some loss trend indicators\n",
    "from collections import deque\n",
    "weighted_average = deque([], maxlen=20)\n",
    "if running_loss_log == []:\n",
    "    running_loss = 1.\n",
    "\n",
    "# This is the main training loop\n",
    "for i in range(100):\n",
    "    # At each new epoch we generate new points (its like a new batch), but we make sure\n",
    "    # they are outside the unit cube\n",
    "    targets = (torch.rand(batch_size,3)*2-1)*1.1\n",
    "    a = torch.logical_and((targets[:,0]>-1),(targets[:,0]<1))\n",
    "    b = torch.logical_and((targets[:,1]>-1),(targets[:,1]<1))\n",
    "    c = torch.logical_and((targets[:,2]>-1),(targets[:,2]<1))\n",
    "    d = torch.logical_and(torch.logical_or(a,b), c)\n",
    "    targets=targets[d]\n",
    "    labels = gravann.U_L(targets, points, masses)\n",
    "    \n",
    "    # Compute the loss (use N=3000 to start with, then, eventually, beef it up to 200000)\n",
    "    predicted = mc_method(targets, model, encoding, N=N_mc_points)\n",
    "    loss = loss_fn(predicted, labels)\n",
    "    # Update the loss trend indicators\n",
    "    running_loss = 0.9 * running_loss + 0.1 * loss.item()\n",
    "    weighted_average.append(loss.item())\n",
    "    # Update the logs\n",
    "    running_loss_log.append(running_loss)\n",
    "    weighted_average_log.append(np.mean(weighted_average))\n",
    "    loss_log.append(loss.item())\n",
    "    n_inferences.append(N_mc_points*batch_size)\n",
    "    # Print every i iterations\n",
    "    if i % 1 == 0:\n",
    "        wa_out = np.mean(weighted_average)\n",
    "        print(f\"It={i}\\t loss={loss.item():.6f}\\t  weighted_average={wa_out:.6f}\\t  running_loss={running_loss:.6f}\")\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Perform a step in LR scheduler to update LR\n",
    "    scheduler.step(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gravann.plot_model_rejection(model, encoding, views_2d=True, bw=True, N=50000, crop_p=0.1, alpha=0.1, s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the loss history\n",
    "plt.figure()\n",
    "abscissa = np.cumsum(n_inferences)\n",
    "plt.semilogy(abscissa, loss_log)\n",
    "plt.semilogy(abscissa, running_loss_log)\n",
    "plt.semilogy(abscissa, weighted_average_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gravann.plot_model_mesh(model,encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save to models/cluster_xxxx\n",
    "#torch.save(model.state_dict(), \"models/\" + name + \"_\" + encoding.name + \".mdl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO list:\n",
    "\n",
    "* Code efficiency -> move to GPU and make training scalable to more sample points / mc points.\n",
    "* MC integration -> importance sampling maybe?\n",
    "* Network architecture -> study different encodings\n",
    "* How to visualize and interpret the results quantitatively.\n",
    "* Propagate trajectories around the asteroids (ground truth and trained).\n",
    "* Incorporate visual cues.\n",
    "* Training with gravity rather than potential?\n",
    "* What happens for non uniform bodies?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
